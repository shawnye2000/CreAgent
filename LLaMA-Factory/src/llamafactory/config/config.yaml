# name of LLM model, 'custom' for custom model
llm: custom
api_key: xxx
api_base: xxx
llm_name: xxx
# path for item data
item_save_path: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/youtube/1012_saved_item_FairAgent_ranking_MF_TFROM_start_step0_2024_repeat_1.json
# path for user data
user_path: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/youtube/users.json   #data/small_user.csv    #user_df.csv
# path for provider data
provider_path: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/youtube/providers.json     #data/provider_profile.json
# path for save interaction records
profile_path: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/youtube/provider_profile.json
# path for save interaction records
interaction_path: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/data/1012_interaction_FairAgent_ranking_MF_reranking_TFROM_recency_10_2024_repeat_1.csv
# directory name for faiss index
ckpt_path: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/data/ckpt
index_name: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/data/faiss_index
# simulator directory name for saving and restoring
simulator_dir: /home/xxx/experiment/Agent4Fairness/LLaMA-Factory/data/data/simulator
# simulator restoring name
simulator_restore_file_name:
# random seed
seed: 2024  #  1. 1  2. 2024
# recommender system model
rec_model: MF  #Pop
# re-ranking system model
reranking_model: TFROM  #TFROM   #False  #P_MMF  #FairRec  #False    #MF
tradeoff_para: 0.5 #100
# train epoch number for recommender system model
epoch_num: 60
# batch size for recommender system model
batch_size: 1024
# embedding size for recommender system model
embedding_size: 64
# learning rate for recommender system model
#lr: 0.001
# number of epochs
round: 110
# number of agents, which cannot exceed the number of user in user.csv
active_agent_threshold: 1500 #110
# method for choose agent to be active, random, sample, or marginal
agent_num:  1500  #100
# number of provider agents, which cannot exceed the number of providers in provider.csv
provider_agent_num:  50
# temperature for LLM
temperature: 0
# maximum number of tokens for LLM
max_token: 1500
# execution mode, serial or parallel
execution_mode: parallel  #serial  #
# time interval for action of agents. The form is a number + a single letter, and the letter may be s, m, h, d, M, y
interval: 5h
# number of max retries for LLM API
max_retries: 100
# verbose mode
verbose: True
# threshold for agent num

active_method: random
# propability for agent to be active
active_prob: 0.5

active_proagent_threshold: 50
proagent_active_method: random
# implementation of agent memory, recagent or GenerativeAgentMemory
agent_memory: recagent
# list of api keys for LLM API
api_keys: EMPTY
# ratio of random recommendation, 1,3,5
rec_random_k: 0

max_item_num: 100000

#create_new_content: True

TopK: 5  # length of user recommendation list

item_recency: 10  #5  #5 # only recommend the most recent 5 rounds item
finetune: False #True
provider_decision_policy: bounded_rational #full_information #CFD    #LBR  #  #LBR #full_information  #   #bounded_rational  # choosen from "random","full_information", "bounded_rational", "UWO", "PPA", "EcoAgent"
provider_lr: 0.1
provider_decision_mode: click_based
# rec hyper
lr: 0.001
weight_decay: 0.0001


max_len: 50

user_id_dim: 32
item_id_dim: 32
item_brand_id_dim: 32
item_cate_id_dim: 32

hid_units: [200, 80, 1]
dropout: 0.1

with_rec: True
with_leave: True

# dcn
mlp_hidden_size: [200, 80]
cross_layer_num: 3
dropout_prob: 0.1


# max_seq_len
max_seq_len: 20


max_workers: 4


reranking_start_step: 10

# whether have decision making under information assymetry
provider_decision_making: True

eval_agent_num: 1500
